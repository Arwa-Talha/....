{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"colab":{"provenance":[]},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport shutil\n\nworking_dir = \"/kaggle/working\"\n\n# List all items in the working directory\nfor item in os.listdir(working_dir):\n    item_path = os.path.join(working_dir, item)\n    try:\n        # Remove directories recursively\n        if os.path.isdir(item_path):\n            shutil.rmtree(item_path)\n            print(f\"Deleted directory: {item_path}\")\n        # Remove files\n        else:\n            os.remove(item_path)\n            print(f\"Deleted file: {item_path}\")\n    except Exception as e:\n        print(f\"Error deleting {item_path}: {e}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!git clone https://github.com/ibaiGorordo/Ultrafast-Lane-Detection-Inference-Pytorch-.git\n!cat /kaggle/working/Ultrafast-Lane-Detection-Inference-Pytorch-/ultrafastLaneDetector/ultrafastLaneDetector.py","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"enhanced_code = '''\n# --- FCWS Safe Distance Parameters ---\nbrake_buffer_by_class = {\n    \"car\": 10,\n    \"truck\": 20,\n    \"bus\": 18,\n    \"motorcycle\": 8,\n    \"person\": 8,\n    \"unknown\": 12\n}\nreaction_time = 1.5  # in seconds\n\ndef compute_safe_distance(object_class, current_speed_kmph):\n    speed_mps = current_speed_kmph / 3.6\n    buffer = brake_buffer_by_class.get(object_class.lower(), brake_buffer_by_class[\"unknown\"])\n    return round((reaction_time * speed_mps) + buffer, 2)\n\n\nimport cv2\nimport torch\nimport time\nimport numpy as np\nimport torchvision.transforms as transforms\nfrom PIL import Image\nfrom enum import Enum\nimport scipy.special\nfrom scipy.interpolate import UnivariateSpline\nfrom collections import deque \nfrom ultrafastLaneDetector.model import parsingNet\n\nlane_colors = [(0,0,255), (0,255,0), (255,0,0), (0,255,255)]\n\ntusimple_row_anchor = [64, 68, 72, 76, 80, 84, 88, 92, 96, 100, 104, 108, 112,\n            116, 120, 124, 128, 132, 136, 140, 144, 148, 152, 156, 160, 164,\n            168, 172, 176, 180, 184, 188, 192, 196, 200, 204, 208, 212, 216,\n            220, 224, 228, 232, 236, 240, 244, 248, 252, 256, 260, 264, 268,\n            272, 276, 280, 284]\nculane_row_anchor = [121, 131, 141, 150, 160, 170, 180, 189, 199, 209, 219, 228, 238, 248, 258, 267, 277, 287]\n\nclass ModelType(Enum):\n    TUSIMPLE = 0\n    CULANE = 1\n\nclass ModelConfig():\n    def __init__(self, model_type):\n        if model_type == ModelType.TUSIMPLE:\n            self.init_tusimple_config()\n        else:\n            self.init_culane_config()\n\n    def init_tusimple_config(self):\n        self.img_w = 1280\n        self.img_h = 720\n        self.row_anchor = tusimple_row_anchor\n        self.griding_num = 100\n        self.cls_num_per_lane = 56\n\n    def init_culane_config(self):\n        self.img_w = 1640\n        self.img_h = 590\n        self.row_anchor = culane_row_anchor\n        self.griding_num = 200\n        self.cls_num_per_lane = 18\n\nclass UltrafastLaneDetector():\n    def __init__(self, model_path, model_type=ModelType.TUSIMPLE, use_gpu=False):\n        self.use_gpu = use_gpu\n        self.cfg = ModelConfig(model_type)\n        self.model = self.initialize_model(model_path, self.cfg, use_gpu)\n        self.img_transform = self.initialize_image_transform()\n        self.lane_history = deque(maxlen=15)\n        self.load_ui_elements()\n        self.detected_objects = []  # Store detected objects for FCWS\n\n    def load_ui_elements(self):\n        \"\"\" Load UI icons for Lane Keeping, Lane Change Assist, and Forward Collision Warning System. \"\"\"\n        self.ui_icons = {\n            \"right_turn\": cv2.imread(\"/kaggle/input/warning-departure/right_turn.png\", cv2.IMREAD_UNCHANGED),\n            \"left_turn\": cv2.imread(\"/kaggle/input/warning-departure/left_turn.png\", cv2.IMREAD_UNCHANGED),\n            \"straight\": cv2.imread(\"/kaggle/input/warning-departure/straight.png\", cv2.IMREAD_UNCHANGED),\n            \"fcws_warning\": cv2.imread(\"/kaggle/input/warning-departure/FCWS-warning.png\", cv2.IMREAD_UNCHANGED),\n            \"fcws_prompt\": cv2.imread(\"/kaggle/input/warning-departure/FCWS-prompt.png\", cv2.IMREAD_UNCHANGED),\n            \"fcws_normal\": cv2.imread(\"/kaggle/input/warning-departure/FCWS-normal.png\", cv2.IMREAD_UNCHANGED),\n            \"lta_left\": cv2.imread(\"/kaggle/input/warning-departure/LTA-left_lanes.png\", cv2.IMREAD_UNCHANGED),\n            \"lta_right\": cv2.imread(\"/kaggle/input/warning-departure/LTA-right_lanes.png\", cv2.IMREAD_UNCHANGED),\n            \"warning\": cv2.imread(\"/kaggle/input/warning-departure/warn.png\", cv2.IMREAD_UNCHANGED),\n        }\n\n    @staticmethod\n    def initialize_model(model_path, cfg, use_gpu):\n        net = parsingNet(pretrained=False, backbone='18', cls_dim=(cfg.griding_num+1, cfg.cls_num_per_lane, 4), use_aux=False)\n        map_loc = 'cuda' if use_gpu and torch.cuda.is_available() else 'cpu'\n        state_dict = torch.load(model_path, map_location=map_loc)['model']\n        compatible_state_dict = {k[7:] if 'module.' in k else k: v for k, v in state_dict.items()}\n        net.load_state_dict(compatible_state_dict, strict=False)\n        net.eval()\n        if use_gpu and not torch.backends.mps.is_built():\n            net = net.cuda()\n        return net\n        \n    @staticmethod    \n    def initialize_image_transform():\n        return transforms.Compose([\n            transforms.Resize((288, 800)),\n            transforms.ToTensor(),\n            transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n        ])\n\n    def overlay_ui(self, frame, fps=0.0):\n        \"\"\"\n        Improved overlay with correctly aligned text and icons.\n        \"\"\"\n        h, w, _ = frame.shape\n    \n        # LKA Status\n        offset = self.compute_lane_offset()\n        if abs(offset) < 20:\n            lka_icon = self.ui_icons[\"straight\"]\n            lka_text_lines = [\"LDWS: Good Lane Keeping\", \"LKAS: Keep Straight Ahead\"]\n            lka_color = (0, 255, 0)\n        elif offset > 20:\n            lka_icon = self.ui_icons[\"right_turn\"]\n            lka_text_lines = [\"⚠ Lane Departure Right\"]\n            lka_color = (0, 0, 255)\n        else:\n            lka_icon = self.ui_icons[\"left_turn\"]\n            lka_text_lines = [\"⚠ Lane Departure Left\"]\n            lka_color = (0, 0, 255)\n\n        # FCWS\n        fcws_icon = self.ui_icons[\"fcws_normal\"]\n        fcws_text = \"FCWS: Normal Risk\"\n        if self.detect_collision_risk(current_speed_kmph=60):  # Or any dynamic value if you estimate speed\n            fcws_icon = self.ui_icons[\"fcws_warning\"]\n            fcws_text = \"⚠ FCWS: High Risk\"\n    \n        # LCA\n        if self.lanes_detected[0]:\n            lca_icon = self.ui_icons[\"lta_left\"]\n        elif self.lanes_detected[3]:\n            lca_icon = self.ui_icons[\"lta_right\"]\n        else:\n            lca_icon = self.ui_icons[\"warning\"]\n    \n        # === TOP LEFT: LKA BOX ===\n        cv2.rectangle(frame, (30, 30), (280, 170), (0, 0, 0), 2)\n        frame = self.overlay_icon(frame, lka_icon, (100, 50))\n        for i, line in enumerate(lka_text_lines):\n            cv2.putText(frame, line, (50, 155 + i*30), cv2.FONT_HERSHEY_SIMPLEX, 0.5, lka_color, 2)\n    \n        # === TOP RIGHT: FCWS BOX ===\n        cv2.rectangle(frame, (w - 250, 30), (w - 30, 240), (0, 0, 0), 2)\n        frame = self.overlay_icon(frame, fcws_icon, (w - 225, 40))\n        cv2.putText(frame, fcws_text, (w - 225, 225), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n    \n        # === LCA Icon (Below LKA) ===\n        frame = self.overlay_icon(frame, lca_icon, (50, 180))\n    \n        # === Bottom Left: FPS ===\n        cv2.putText(frame, f\"FPS: {fps:.2f}\", (60, h - 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)\n    \n        return frame\n\n    def overlay_icon(self, frame, icon, position):\n        \"\"\"\n        Overlay an icon with alpha transparency at the given position.\n        \"\"\"\n        if icon is None:\n            return frame  # Skip if image is not loaded\n    \n        h, w, _ = icon.shape\n        x, y = position\n    \n        # Ensure it fits in the frame\n        if y + h > frame.shape[0] or x + w > frame.shape[1]:\n            return frame\n    \n        # Extract the alpha channel\n        alpha_channel = icon[:, :, 3] / 255.0\n        for c in range(3):  # Apply for each RGB channel\n            frame[y:y+h, x:x+w, c] = (1 - alpha_channel) * frame[y:y+h, x:x+w, c] + alpha_channel * icon[:, :, c]\n        \n        return frame\n\n    def prepare_input(self, img):\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        img_pil = Image.fromarray(img)\n        input_img = self.img_transform(img_pil)\n        input_tensor = input_img[None, ...]\n        if self.use_gpu and not torch.backends.mps.is_built():\n            input_tensor = input_tensor.cuda()\n        return input_tensor\n\n    def inference(self, input_tensor):\n        with torch.no_grad():\n            return self.model(input_tensor)\n\n    def process_output(self, output):\n        processed = output[0].data.cpu().numpy()\n        processed = processed[:, ::-1, :]\n        prob = scipy.special.softmax(processed[:-1, :, :], axis=0)\n        idx = np.arange(self.cfg.griding_num) + 1\n        loc = np.sum(prob * idx.reshape(-1, 1, 1), axis=0)\n        processed = np.argmax(processed, axis=0)\n        loc[processed == self.cfg.griding_num] = 0\n        processed = loc\n    \n        col_sample = np.linspace(0, 800 - 1, self.cfg.griding_num)\n        col_sample_w = col_sample[1] - col_sample[0]\n    \n        lanes_points, lanes_detected = [], []\n        for lane_num in range(processed.shape[1]):\n            points = []\n            valid_points = processed[:, lane_num] != 0\n    \n            # ✅ Stricter threshold to reduce false lanes\n            if np.sum(valid_points) >= 6:  # You had >2 before — this improves robustness\n                lanes_detected.append(True)\n                for i in range(processed.shape[0]):\n                    if processed[i, lane_num] > 0:\n                        x = int(processed[i, lane_num] * col_sample_w * self.cfg.img_w / 800) - 1\n                        y = int(self.cfg.img_h * (self.cfg.row_anchor[self.cfg.cls_num_per_lane - 1 - i] / 288)) - 1\n                        points.append([x, y])\n            else:\n                lanes_detected.append(False)\n            lanes_points.append(points)\n    \n        # ✅ Apply temporal smoothing\n        self.lane_history.append(lanes_points)\n        smoothed_lanes = self.smooth_lanes()\n    \n        return smoothed_lanes, lanes_detected\n\n    def compute_lane_offset(self):\n        \"\"\"\n        Calculate how far the vehicle is from the lane center.\n        A positive value means deviating right, negative means deviating left.\n        \"\"\"\n        if self.lanes_detected[1] and self.lanes_detected[2]:  # Middle lanes detected\n            left_lane = np.array(self.lanes_points[1])\n            right_lane = np.array(self.lanes_points[2])\n    \n            # Ensure equal length by interpolating shorter lane\n            min_len = min(len(left_lane), len(right_lane))\n            left_lane, right_lane = left_lane[:min_len], right_lane[:min_len]\n    \n            # Compute lane center\n            lane_center_x = np.mean((left_lane[:, 0] + right_lane[:, 0]) / 2)\n            car_position_x = self.cfg.img_w // 2  # Assume car is at image center\n    \n            # Compute offset (negative = left, positive = right)\n            offset = car_position_x - lane_center_x\n            return offset\n        return 0  # Default, if lanes not detected\n\n    def get_lka_status(self):\n        \"\"\"\n        Determines if the vehicle is centered in the lane or deviating.\n        \"\"\"\n        offset = self.compute_lane_offset()\n    \n        if abs(offset) < 20:  # Small deviation threshold\n            return \"LDWS: Good Lane Keeping\", (0, 255, 0)  # Green safe zone\n        elif offset > 20:\n            return \"⚠ Lane Departure Right\", (0, 0, 255)  # Red warning\n        else:\n            return \"⚠ Lane Departure Left\", (0, 0, 255)\n\n    def get_lca_status(self):\n        \"\"\"\n        Determines if the driver can change lanes.\n        \"\"\"\n        suggestions = []\n        \n        if self.lanes_detected[0]:  # Left lane detected\n            suggestions.append(\"✅ Left Lane Available\")\n        if self.lanes_detected[3]:  # Right lane detected\n            suggestions.append(\"✅ Right Lane Available\")\n    \n        if not suggestions:\n            suggestions.append(\"❌ No Safe Lane Change\")\n    \n        return suggestions\n\n    def detect_collision_risk(self, current_speed_kmph=60):\n        \"\"\"\n        Smarter FCWS using class-aware safe distance.\n        Only considers forward objects in ego lane.\n        \"\"\"\n        ego_lane = self.get_ego_lane_index()\n        for obj in self.detected_objects:\n            if obj.get(\"lane_idx\") == ego_lane:\n                obj_type = obj[\"type\"]\n                dist = obj[\"distance\"]\n                safe_dist = compute_safe_distance(obj_type, current_speed_kmph)\n                if dist < safe_dist:\n                    return True\n        return False\n\n    def get_ego_lane_index(self):\n        \"\"\"\n        Returns the index of the ego lane (center lane).\n        \"\"\"\n        img_center = self.cfg.img_w // 2\n        lane_centers = []\n        for i, lane in enumerate(self.lanes_points[:4]):\n            if len(lane) > 0:\n                avg_x = np.mean([pt[0] for pt in lane])\n                lane_centers.append((i, avg_x))\n        sorted_lanes = sorted(lane_centers, key=lambda x: abs(x[1] - img_center))\n        return sorted_lanes[0][0] if sorted_lanes else 1  # fallback to lane 1\n    \n    def detect_lanes(self, image, draw_points=True):\n        \"\"\"\n        Detects lanes, applies lane smoothing, and overlays LKA/LCA and FCWS UI elements.\n        Automatically resizes input to model resolution and restores it back.\n        Also computes and displays real-time FPS.\n        \"\"\"\n        # Initialize prev_time if not present\n        if not hasattr(self, 'prev_time'):\n            self.prev_time = time.time()\n            fps = 0.0  # Skip FPS calculation for the first frame\n        else:\n            current_time = time.time()\n            delta = current_time - self.prev_time\n            fps = 1.0 / delta if delta > 0.001 else 0.0  # Prevent division by very small number\n            self.prev_time = current_time\n    \n        # Resize image to model input size\n        original_shape = image.shape[1], image.shape[0]  # (w, h)\n        resized = cv2.resize(image, (self.cfg.img_w, self.cfg.img_h), interpolation=cv2.INTER_AREA)\n    \n        input_tensor = self.prepare_input(resized)\n        output = self.inference(input_tensor)\n        self.lanes_points, self.lanes_detected = self.process_output(output)\n    \n        # Draw lanes\n        vis = self.draw_lanes(resized, self.lanes_points, self.lanes_detected, self.cfg, draw_points)\n    \n        # Overlay UI including FPS\n        vis = self.overlay_ui(vis, fps)\n    \n        # Resize back to original size\n        vis = cv2.resize(vis, original_shape, interpolation=cv2.INTER_LINEAR)\n        return vis, fps\n    \n    def smooth_lanes(self, alpha=0.8):\n        if len(self.lane_history) < 2:\n            return self.lane_history[-1]\n    \n        smoothed_lanes = []\n        for i in range(len(self.lane_history[-1])):\n            weighted = np.array(self.lane_history[-1][i], dtype=np.float32)\n            count = 1\n    \n            for past_frame in list(self.lane_history)[-2::-1]:\n                if i >= len(past_frame):\n                    continue\n                prev = np.array(past_frame[i], dtype=np.float32)\n                if len(prev) != len(weighted):\n                    continue\n                weighted = (alpha * weighted + (1 - alpha) * prev)\n                count += 1\n                if count >= 3:\n                    break\n    \n            smoothed_lanes.append(weighted.astype(int).tolist())\n        return smoothed_lanes\n\n    def draw_lanes(self, input_img, lanes_points, lanes_detected, cfg, draw_points=True):\n        vis = input_img.copy()\n        lane_mask = vis.copy()\n        \n        detected_idxs = [i for i, d in enumerate(lanes_detected) if d]\n        if len(detected_idxs) < 2:\n            return vis\n    \n        # === 1. Compute average x for all detected lanes ===\n        avg_xs = []\n        for idx in detected_idxs:\n            xs = [pt[0] for pt in lanes_points[idx]]\n            avg_x = np.mean(xs) if xs else float('inf')\n            avg_xs.append((idx, avg_x))\n    \n        # === 2. Sort lanes by avg x (from left to right) ===\n        sorted_lanes = sorted(avg_xs, key=lambda x: x[1])\n        sorted_idxs = [idx for idx, _ in sorted_lanes]\n    \n        # === 3. Target lane: Rightmost → largest avg x ===\n        target_idx = sorted_idxs[-1]  # rightmost\n        target_pos = sorted_idxs.index(target_idx)\n    \n        if target_pos == 0:\n            target_pair = (sorted_idxs[0], sorted_idxs[1])\n        else:\n            target_pair = (sorted_idxs[target_pos - 1], sorted_idxs[target_pos])\n\n        # === 4. Ego lane: Find the pair that surrounds image center ===\n        img_center = vis.shape[1] // 2\n        min_gap = float('inf')\n        ego_pair = None\n    \n        for i in range(len(sorted_idxs) - 1):\n            x1 = np.mean([pt[0] for pt in lanes_points[sorted_idxs[i]]])\n            x2 = np.mean([pt[0] for pt in lanes_points[sorted_idxs[i + 1]]])\n            if x1 < img_center < x2:\n                gap = abs((x1 + x2) / 2 - img_center)\n                if gap < min_gap:\n                    min_gap = gap\n                    ego_pair = (sorted_idxs[i], sorted_idxs[i + 1])\n    \n        # fallback: if nothing surrounds center, pick mid-pair\n        if ego_pair is None:\n            mid = len(sorted_idxs) // 2\n            ego_pair = (sorted_idxs[mid - 1], sorted_idxs[mid])\n    \n        # === 5. Color assignment ===\n        is_same = (set(ego_pair) == set(target_pair))\n        \n        # Ego lane (🟡 or 🟢)\n        ego_color = (0, 255, 0) if is_same else (255, 191, 0)\n        cv2.fillPoly(\n            lane_mask,\n            [np.vstack((lanes_points[ego_pair[0]], np.flipud(lanes_points[ego_pair[1]])))],\n            color=ego_color\n        )\n\n        # Target lane (🔴 if not ego)\n        if not is_same:\n            cv2.fillPoly(\n                lane_mask,\n                [np.vstack((lanes_points[target_pair[0]], np.flipud(lanes_points[target_pair[1]])))],\n                color=(0, 0, 255)\n            )\n    \n        # === 6. Overlay blended mask ===\n        vis = cv2.addWeighted(vis, 0.7, lane_mask, 0.3, 0)\n    \n        # === 7. Draw lane points ===\n        if draw_points:\n            for i, lane in enumerate(lanes_points):\n                smoothed = self.fit_lane_curve(lane)\n                for pt in smoothed:\n                    cv2.circle(vis, tuple(pt), 3, lane_colors[i % len(lane_colors)], -1)\n    \n        return vis\n            \n    def fit_lane_curve(self, lane_points):\n        \"\"\"\n        Smooth lane points using a Univariate Spline for better curve adaptation.\n        \"\"\"\n        if len(lane_points) < 5:  # Avoid error if too few points\n            return lane_points\n        \n        lane_points = np.array(lane_points)\n        x, y = lane_points[:, 0], lane_points[:, 1]\n    \n        try:\n            spline = UnivariateSpline(y, x, k=2, s=3)\n            y_new = np.linspace(y.min(), y.max(), 100)\n            x_new = spline(y_new)\n            return list(zip(x_new.astype(int), y_new.astype(int)))\n        except:\n            return lane_points  # Return original points if spline fails\n\n    def suggest_lane_change(self):\n        \"\"\"\n        Suggest if the driver can safely change lanes.\n        \"\"\"\n        suggestions = []\n        lane_width_threshold = 3.5  # Example lane width in meters\n\n        if self.lanes_detected[0]:\n            suggestions.append(\"✅ Left lane available for a lane change.\")\n        if self.lanes_detected[3]:\n            suggestions.append(\"✅ Right lane available for a lane change.\")\n\n        return suggestions\n\n    def get_center_line(self):\n        \"\"\"\n        Compute the centerline between left and right lanes.\n        \"\"\"\n        if self.lanes_detected[1] and self.lanes_detected[2]:\n            l, r = self.lanes_points[1], self.lanes_points[2]\n            if len(l) == len(r):\n                return ((np.array(l) + np.array(r)) // 2).tolist()\n        return []\n'''\n\n# Overwrite the file\nwith open(\"/kaggle/working/Ultrafast-Lane-Detection-Inference-Pytorch-/ultrafastLaneDetector/ultrafastLaneDetector.py\", \"w\") as f:\n    f.write(enhanced_code)\n\nprint(\"✅ ultrafastLaneDetector.py has been updated with enhancements.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nos._exit(00)  \nimport sys\nimport os\n\n# Define the repository path\nrepo_path = \"/kaggle/working/Ultrafast-Lane-Detection-Inference-Pytorch-\"\n\n# Add the repo path to sys.path\nsys.path.append(repo_path)\n\n# Verify the module path\nprint(\"Repo path added:\", repo_path)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ✅ Set device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# ✅ Load model and move to GPU if available\nmodel_path = \"/kaggle/input/culane-18/culane_18.pth\"\nlane_detector = UltrafastLaneDetector(model_path, model_type=ModelType.CULANE, use_gpu=(device.type == \"cuda\"))\nlane_detector.model.to(device)\nprint(\"✅ Model loaded successfully!\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"yolo_model = YOLO(\"/kaggle/input/yolo11l/yolo11l.pt\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Install required packages\n!pip install wget\n!pip install open3d\n!pip install ultralytics\n!pip install matplotlib\n!pip install torch\n!pip install torchvision\n!pip install deep-sort-realtime\n!pip install torchreid\n!pip install scipy","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BuWO_zLD2_le","outputId":"4a409106-905c-482f-a91d-1ec540ad11ed","collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting wget\n","  Downloading wget-3.2.zip (10 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Building wheels for collected packages: wget\n","  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9655 sha256=c65ae2d389a08eccac5cce162277c7f31f078362d17fc2256db3b52eec38ac9b\n","  Stored in directory: /root/.cache/pip/wheels/40/b3/0f/a40dbd1c6861731779f62cc4babcb234387e11d697df70ee97\n","Successfully built wget\n","Installing collected packages: wget\n","Successfully installed wget-3.2\n","Collecting open3d\n","  Downloading open3d-0.19.0-cp311-cp311-manylinux_2_31_x86_64.whl.metadata (4.3 kB)\n","Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.11/dist-packages (from open3d) (2.0.2)\n","Collecting dash>=2.6.0 (from open3d)\n","  Downloading dash-3.0.3-py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: werkzeug>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from open3d) (3.1.3)\n","Requirement already satisfied: flask>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from open3d) (3.1.0)\n","Requirement already satisfied: nbformat>=5.7.0 in /usr/local/lib/python3.11/dist-packages (from open3d) (5.10.4)\n","Collecting configargparse (from open3d)\n","  Downloading ConfigArgParse-1.7-py3-none-any.whl.metadata (23 kB)\n","Collecting ipywidgets>=8.0.4 (from open3d)\n","  Downloading ipywidgets-8.1.6-py3-none-any.whl.metadata (2.4 kB)\n","Collecting addict (from open3d)\n","  Downloading addict-2.4.0-py3-none-any.whl.metadata (1.0 kB)\n","Requirement already satisfied: pillow>=9.3.0 in /usr/local/lib/python3.11/dist-packages (from open3d) (11.1.0)\n","Requirement already satisfied: matplotlib>=3 in /usr/local/lib/python3.11/dist-packages (from open3d) (3.10.0)\n","Requirement already satisfied: pandas>=1.0 in /usr/local/lib/python3.11/dist-packages (from open3d) (2.2.2)\n","Requirement already satisfied: pyyaml>=5.4.1 in /usr/local/lib/python3.11/dist-packages (from open3d) (6.0.2)\n","Requirement already satisfied: scikit-learn>=0.21 in /usr/local/lib/python3.11/dist-packages (from open3d) (1.6.1)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from open3d) (4.67.1)\n","Collecting pyquaternion (from open3d)\n","  Downloading pyquaternion-0.9.9-py3-none-any.whl.metadata (1.4 kB)\n","Collecting flask>=3.0.0 (from open3d)\n","  Downloading flask-3.0.3-py3-none-any.whl.metadata (3.2 kB)\n","Collecting werkzeug>=3.0.0 (from open3d)\n","  Downloading werkzeug-3.0.6-py3-none-any.whl.metadata (3.7 kB)\n","Requirement already satisfied: plotly>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from dash>=2.6.0->open3d) (5.24.1)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.11/dist-packages (from dash>=2.6.0->open3d) (8.6.1)\n","Requirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.11/dist-packages (from dash>=2.6.0->open3d) (4.13.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from dash>=2.6.0->open3d) (2.32.3)\n","Collecting retrying (from dash>=2.6.0->open3d)\n","  Downloading retrying-1.3.4-py3-none-any.whl.metadata (6.9 kB)\n","Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.11/dist-packages (from dash>=2.6.0->open3d) (1.6.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from dash>=2.6.0->open3d) (75.2.0)\n","Requirement already satisfied: Jinja2>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from flask>=3.0.0->open3d) (3.1.6)\n","Requirement already satisfied: itsdangerous>=2.1.2 in /usr/local/lib/python3.11/dist-packages (from flask>=3.0.0->open3d) (2.2.0)\n","Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.11/dist-packages (from flask>=3.0.0->open3d) (8.1.8)\n","Requirement already satisfied: blinker>=1.6.2 in /usr/local/lib/python3.11/dist-packages (from flask>=3.0.0->open3d) (1.9.0)\n","Collecting comm>=0.1.3 (from ipywidgets>=8.0.4->open3d)\n","  Downloading comm-0.2.2-py3-none-any.whl.metadata (3.7 kB)\n","Requirement already satisfied: ipython>=6.1.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets>=8.0.4->open3d) (7.34.0)\n","Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.11/dist-packages (from ipywidgets>=8.0.4->open3d) (5.7.1)\n","Collecting widgetsnbextension~=4.0.14 (from ipywidgets>=8.0.4->open3d)\n","  Downloading widgetsnbextension-4.0.14-py3-none-any.whl.metadata (1.6 kB)\n","Collecting jupyterlab_widgets~=3.0.14 (from ipywidgets>=8.0.4->open3d)\n","  Downloading jupyterlab_widgets-3.0.14-py3-none-any.whl.metadata (4.1 kB)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3->open3d) (1.3.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3->open3d) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3->open3d) (4.57.0)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3->open3d) (1.4.8)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3->open3d) (24.2)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3->open3d) (3.2.3)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3->open3d) (2.8.2)\n","Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.11/dist-packages (from nbformat>=5.7.0->open3d) (2.21.1)\n","Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.11/dist-packages (from nbformat>=5.7.0->open3d) (4.23.0)\n","Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /usr/local/lib/python3.11/dist-packages (from nbformat>=5.7.0->open3d) (5.7.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0->open3d) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0->open3d) (2025.2)\n","Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.21->open3d) (1.14.1)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.21->open3d) (1.4.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.21->open3d) (3.6.0)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=3.0.0->open3d) (3.0.2)\n","Collecting jedi>=0.16 (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d)\n","  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (4.4.2)\n","Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.7.5)\n","Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (3.0.50)\n","Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (2.18.0)\n","Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.2.0)\n","Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.1.7)\n","Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (4.9.0)\n","Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat>=5.7.0->open3d) (25.3.0)\n","Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat>=5.7.0->open3d) (2024.10.1)\n","Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat>=5.7.0->open3d) (0.36.2)\n","Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat>=5.7.0->open3d) (0.24.0)\n","Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.11/dist-packages (from jupyter-core!=5.0.*,>=4.12->nbformat>=5.7.0->open3d) (4.3.7)\n","Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly>=5.0.0->dash>=2.6.0->open3d) (9.1.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3->open3d) (1.17.0)\n","Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata->dash>=2.6.0->open3d) (3.21.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->dash>=2.6.0->open3d) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->dash>=2.6.0->open3d) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->dash>=2.6.0->open3d) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->dash>=2.6.0->open3d) (2025.1.31)\n","Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.8.4)\n","Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.7.0)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.2.13)\n","Downloading open3d-0.19.0-cp311-cp311-manylinux_2_31_x86_64.whl (447.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m447.7/447.7 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading dash-3.0.3-py3-none-any.whl (8.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.0/8.0 MB\u001b[0m \u001b[31m72.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading flask-3.0.3-py3-none-any.whl (101 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading ipywidgets-8.1.6-py3-none-any.whl (139 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.8/139.8 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading werkzeug-3.0.6-py3-none-any.whl (227 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m228.0/228.0 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading addict-2.4.0-py3-none-any.whl (3.8 kB)\n","Downloading ConfigArgParse-1.7-py3-none-any.whl (25 kB)\n","Downloading pyquaternion-0.9.9-py3-none-any.whl (14 kB)\n","Downloading comm-0.2.2-py3-none-any.whl (7.2 kB)\n","Downloading jupyterlab_widgets-3.0.14-py3-none-any.whl (213 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m214.0/214.0 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading widgetsnbextension-4.0.14-py3-none-any.whl (2.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m83.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading retrying-1.3.4-py3-none-any.whl (11 kB)\n","Downloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m62.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: addict, widgetsnbextension, werkzeug, retrying, pyquaternion, jupyterlab_widgets, jedi, configargparse, comm, flask, ipywidgets, dash, open3d\n","  Attempting uninstall: widgetsnbextension\n","    Found existing installation: widgetsnbextension 3.6.10\n","    Uninstalling widgetsnbextension-3.6.10:\n","      Successfully uninstalled widgetsnbextension-3.6.10\n","  Attempting uninstall: werkzeug\n","    Found existing installation: Werkzeug 3.1.3\n","    Uninstalling Werkzeug-3.1.3:\n","      Successfully uninstalled Werkzeug-3.1.3\n","  Attempting uninstall: jupyterlab_widgets\n","    Found existing installation: jupyterlab_widgets 3.0.13\n","    Uninstalling jupyterlab_widgets-3.0.13:\n","      Successfully uninstalled jupyterlab_widgets-3.0.13\n","  Attempting uninstall: flask\n","    Found existing installation: Flask 3.1.0\n","    Uninstalling Flask-3.1.0:\n","      Successfully uninstalled Flask-3.1.0\n","  Attempting uninstall: ipywidgets\n","    Found existing installation: ipywidgets 7.7.1\n","    Uninstalling ipywidgets-7.7.1:\n","      Successfully uninstalled ipywidgets-7.7.1\n","Successfully installed addict-2.4.0 comm-0.2.2 configargparse-1.7 dash-3.0.3 flask-3.0.3 ipywidgets-8.1.6 jedi-0.19.2 jupyterlab_widgets-3.0.14 open3d-0.19.0 pyquaternion-0.9.9 retrying-1.3.4 werkzeug-3.0.6 widgetsnbextension-4.0.14\n","Collecting ultralytics\n","  Downloading ultralytics-8.3.111-py3-none-any.whl.metadata (37 kB)\n","Requirement already satisfied: numpy<=2.1.1,>=1.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.0.2)\n","Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (3.10.0)\n","Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.11.0.86)\n","Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (11.1.0)\n","Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (6.0.2)\n","Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.32.3)\n","Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (1.14.1)\n","Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.6.0+cu124)\n","Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (0.21.0+cu124)\n","Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.67.1)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ultralytics) (5.9.5)\n","Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from ultralytics) (9.0.0)\n","Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.2.2)\n","Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (0.13.2)\n","Collecting ultralytics-thop>=2.0.0 (from ultralytics)\n","  Downloading ultralytics_thop-2.0.14-py3-none-any.whl.metadata (9.4 kB)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.57.0)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.8)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (24.2)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.3)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2025.1.31)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.18.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (4.13.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2025.3.2)\n","Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n","  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n","  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n","  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.8.0->ultralytics)\n","  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.8.0->ultralytics)\n","  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.8.0->ultralytics)\n","  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.8.0->ultralytics)\n","  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.8.0->ultralytics)\n","  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.8.0->ultralytics)\n","  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n","Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n","  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.8.0->ultralytics) (1.3.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\n","Downloading ultralytics-8.3.111-py3-none-any.whl (978 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m978.8/978.8 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m86.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m82.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m49.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m87.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading ultralytics_thop-2.0.14-py3-none-any.whl (26 kB)\n","Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, ultralytics-thop, ultralytics\n","  Attempting uninstall: nvidia-nvjitlink-cu12\n","    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n","    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n","      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n","  Attempting uninstall: nvidia-curand-cu12\n","    Found existing installation: nvidia-curand-cu12 10.3.6.82\n","    Uninstalling nvidia-curand-cu12-10.3.6.82:\n","      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n","  Attempting uninstall: nvidia-cufft-cu12\n","    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n","    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n","      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n","  Attempting uninstall: nvidia-cuda-runtime-cu12\n","    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n","    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n","    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n","    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-cupti-cu12\n","    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n","    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n","  Attempting uninstall: nvidia-cublas-cu12\n","    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n","    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n","      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n","  Attempting uninstall: nvidia-cusparse-cu12\n","    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n","    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n","      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n","  Attempting uninstall: nvidia-cudnn-cu12\n","    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n","    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n","      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n","  Attempting uninstall: nvidia-cusolver-cu12\n","    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n","    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n","      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n","Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 ultralytics-8.3.111 ultralytics-thop-2.0.14\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.57.0)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n","Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.0.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n","Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n","Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n","Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n","Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n","Requirement already satisfied: torch==2.6.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.6.0+cu124)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.18.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (4.13.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (2025.3.2)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (9.1.0.70)\n","Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.5.8)\n","Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (11.2.1.3)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (10.3.5.147)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (11.6.1.9)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.3.1.170)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch==2.6.0->torchvision) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.6.0->torchvision) (3.0.2)\n","Collecting deep-sort-realtime\n","  Downloading deep_sort_realtime-1.3.2-py3-none-any.whl.metadata (12 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from deep-sort-realtime) (2.0.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from deep-sort-realtime) (1.14.1)\n","Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (from deep-sort-realtime) (4.11.0.86)\n","Downloading deep_sort_realtime-1.3.2-py3-none-any.whl (8.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m44.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: deep-sort-realtime\n","Successfully installed deep-sort-realtime-1.3.2\n","Collecting torchreid\n","  Downloading torchreid-0.2.5.tar.gz (92 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.7/92.7 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Building wheels for collected packages: torchreid\n","  Building wheel for torchreid (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for torchreid: filename=torchreid-0.2.5-py3-none-any.whl size=144324 sha256=05bc7075b19ee98fee7b19a45c62b2e94054de48c7f1379bef37cfd250487b3b\n","  Stored in directory: /root/.cache/pip/wheels/61/dc/08/b478469bab07b5ede9e962968ebe3c8961c10c5fc106a6c697\n","Successfully built torchreid\n","Installing collected packages: torchreid\n","Successfully installed torchreid-0.2.5\n","Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (1.14.1)\n","Requirement already satisfied: numpy<2.3,>=1.23.5 in /usr/local/lib/python3.11/dist-packages (from scipy) (2.0.2)\n"]}],"execution_count":1},{"cell_type":"code","source":"import os\nimport shutil\nimport sys\nimport numpy as np\nimport cv2\nimport open3d as o3d\nimport matplotlib.pyplot as plt\nimport glob\nimport struct\nimport torch\nimport torchvision.transforms as transforms\nfrom PIL import Image\nfrom ultralytics import YOLO\nfrom deep_sort_realtime.deepsort_tracker import DeepSort\nfrom google.colab import drive\nfrom enum import Enum\nimport scipy.special\nfrom scipy.interpolate import UnivariateSpline\nfrom collections import deque\nimport matplotlib\nmatplotlib.use('Agg')  # Non-interactive backend for video rendering","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":91},"id":"psEmbEcK3BMG","outputId":"e9ef55f2-6f27-4cd7-871b-07a7fc0eebad"},"outputs":[{"output_type":"stream","name":"stdout","text":["Creating new Ultralytics Settings v0.0.6 file ✅ \n","View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n","Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n"]}],"execution_count":2},{"cell_type":"code","source":"# Mount Google Drive\ndrive.mount('/content/drive', force_remount=False)\n%cd /content/drive/My Drive/\ndownload_path = \"/content/drive/MyDrive/CameraLidarFusion\"\nos.makedirs(download_path, exist_ok=True)\n\n# Clear previous contents\nfusion_dir = \"/content/drive/MyDrive/CameraLidarFusion\"\nif os.path.exists(fusion_dir):\n    shutil.rmtree(fusion_dir)\n    print(f\"Cleared all contents of {fusion_dir}\")\nos.makedirs(fusion_dir, exist_ok=True)\nprint(f\"Recreated empty directory {fusion_dir}\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vv4frOxw3UnE","outputId":"a85101b8-a486-40b5-bfc7-f40dab8bf9ea"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/My Drive\n","Cleared all contents of /content/drive/MyDrive/CameraLidarFusion\n","Recreated empty directory /content/drive/MyDrive/CameraLidarFusion\n"]}],"execution_count":3},{"cell_type":"code","source":"# Download KITTI data\n!wget https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0001/2011_09_26_drive_0001_sync.zip -P /content/drive/MyDrive/CameraLidarFusion/\n!wget https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_calib.zip -P /content/drive/MyDrive/CameraLidarFusion/\n!wget https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0001/2011_09_26_drive_0001_tracklets.zip -P /content/drive/MyDrive/CameraLidarFusion/\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z7yNr-H_3X-P","outputId":"8f92b8df-a653-4247-da1f-38479a49c3b3"},"outputs":[{"output_type":"stream","name":"stdout","text":["--2025-04-18 15:03:46--  https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0001/2011_09_26_drive_0001_sync.zip\n","Resolving s3.eu-central-1.amazonaws.com (s3.eu-central-1.amazonaws.com)... 3.5.135.239, 52.219.171.137, 52.219.171.89, ...\n","Connecting to s3.eu-central-1.amazonaws.com (s3.eu-central-1.amazonaws.com)|3.5.135.239|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 458643963 (437M) [application/zip]\n","Saving to: ‘/content/drive/MyDrive/CameraLidarFusion/2011_09_26_drive_0001_sync.zip’\n","\n","2011_09_26_drive_00 100%[===================>] 437.40M  12.2MB/s    in 40s     \n","\n","2025-04-18 15:04:28 (10.9 MB/s) - ‘/content/drive/MyDrive/CameraLidarFusion/2011_09_26_drive_0001_sync.zip’ saved [458643963/458643963]\n","\n","--2025-04-18 15:04:28--  https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_calib.zip\n","Resolving s3.eu-central-1.amazonaws.com (s3.eu-central-1.amazonaws.com)... 3.5.137.216, 52.219.169.197, 52.219.169.69, ...\n","Connecting to s3.eu-central-1.amazonaws.com (s3.eu-central-1.amazonaws.com)|3.5.137.216|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 4068 (4.0K) [application/zip]\n","Saving to: ‘/content/drive/MyDrive/CameraLidarFusion/2011_09_26_calib.zip’\n","\n","2011_09_26_calib.zi 100%[===================>]   3.97K  --.-KB/s    in 0s      \n","\n","2025-04-18 15:04:29 (77.8 MB/s) - ‘/content/drive/MyDrive/CameraLidarFusion/2011_09_26_calib.zip’ saved [4068/4068]\n","\n","--2025-04-18 15:04:29--  https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0001/2011_09_26_drive_0001_tracklets.zip\n","Resolving s3.eu-central-1.amazonaws.com (s3.eu-central-1.amazonaws.com)... 3.5.137.216, 52.219.169.197, 52.219.169.69, ...\n","Connecting to s3.eu-central-1.amazonaws.com (s3.eu-central-1.amazonaws.com)|3.5.137.216|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 280497 (274K) [application/zip]\n","Saving to: ‘/content/drive/MyDrive/CameraLidarFusion/2011_09_26_drive_0001_tracklets.zip’\n","\n","2011_09_26_drive_00 100%[===================>] 273.92K   314KB/s    in 0.9s    \n","\n","2025-04-18 15:04:31 (314 KB/s) - ‘/content/drive/MyDrive/CameraLidarFusion/2011_09_26_drive_0001_tracklets.zip’ saved [280497/280497]\n","\n"]}],"execution_count":4},{"cell_type":"code","source":"# Unzip files\nimport zipfile\nsync_zip = \"/content/drive/MyDrive/CameraLidarFusion/2011_09_26_drive_0001_sync.zip\"\ncalib_zip = \"/content/drive/MyDrive/CameraLidarFusion/2011_09_26_calib.zip\"\ntracklet_zip = \"/content/drive/MyDrive/CameraLidarFusion/2011_09_26_drive_0001_tracklets.zip\"\nfor zip_path in [sync_zip, calib_zip, tracklet_zip]:\n    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n        zip_ref.extractall(\"/content/drive/MyDrive/CameraLidarFusion/\")\n    print(f\"Unzipped {zip_path}\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Tcw39kfk3qjQ","outputId":"de9b3ee6-e7ee-4e78-dfd0-af4ea8dba653"},"outputs":[{"output_type":"stream","name":"stdout","text":["Unzipped /content/drive/MyDrive/CameraLidarFusion/2011_09_26_drive_0001_sync.zip\n","Unzipped /content/drive/MyDrive/CameraLidarFusion/2011_09_26_calib.zip\n","Unzipped /content/drive/MyDrive/CameraLidarFusion/2011_09_26_drive_0001_tracklets.zip\n"]}],"execution_count":5},{"cell_type":"code","source":"# Clone UFLD repository\n!git clone https://github.com/ibaiGorordo/Ultrafast-Lane-Detection-Inference-Pytorch-.git /content/drive/MyDrive/CameraLidarFusion/Ultrafast-Lane-Detection-Inference-Pytorch-\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bJhzj7De3rzr","outputId":"54bcd91e-4ea8-40d8-a73c-8f0438c47d86"},"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into '/content/drive/MyDrive/CameraLidarFusion/Ultrafast-Lane-Detection-Inference-Pytorch-'...\n","remote: Enumerating objects: 63, done.\u001b[K\n","remote: Counting objects: 100% (63/63), done.\u001b[K\n","remote: Compressing objects: 100% (46/46), done.\u001b[K\n","remote: Total 63 (delta 24), reused 39 (delta 9), pack-reused 0 (from 0)\u001b[K\n","Receiving objects: 100% (63/63), 10.88 MiB | 11.24 MiB/s, done.\n","Resolving deltas: 100% (24/24), done.\n"]}],"execution_count":6},{"cell_type":"code","source":"# Clone the repository\n!git clone https://github.com/ibaiGorordo/Ultrafast-Lane-Detection-Inference-Pytorch-.git\n%cd /kaggle/working/Ultrafast-Lane-Detection-Inference-Pytorch-","metadata":{"id":"hRbHDtEzfE-i"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Write UFLD code to file\nos.makedirs(\"/content/drive/MyDrive/CameraLidarFusion/Ultrafast-Lane-Detection-Inference-Pytorch-/ultrafastLaneDetector/\", exist_ok=True)\nwith open(\"/content/drive/MyDrive/CameraLidarFusion/Ultrafast-Lane-Detection-Inference-Pytorch-/ultrafastLaneDetector/ultrafastLaneDetector.py\", \"w\") as f:\n    f.write(ufld_code)\nprint(\"✅ ultrafastLaneDetector.py has been written.\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2hFI_blk2_0s","outputId":"e2e5efa1-f714-4e21-b3f3-3c4008abf390"},"outputs":[{"output_type":"stream","name":"stdout","text":["✅ ultrafastLaneDetector.py has been written.\n"]}],"execution_count":8},{"cell_type":"code","source":"# Add UFLD repo to sys.path\nsys.path.append(\"/content/drive/MyDrive/CameraLidarFusion/Ultrafast-Lane-Detection-Inference-Pytorch-\")\nfrom ultrafastLaneDetector.ultrafastLaneDetector import UltrafastLaneDetector, ModelType\n\n# KITTI data paths\nbase_dir = \"/content/drive/MyDrive/CameraLidarFusion/2011_09_26/2011_09_26_drive_0001_sync\"\ncalib_dir = \"/content/drive/MyDrive/CameraLidarFusion/2011_09_26\"\nimage_files = sorted(glob.glob(f\"{base_dir}/image_02/data/*.png\"))\npoint_files = sorted(glob.glob(f\"{base_dir}/velodyne_points/data/*.bin\"))\ncalib_files = sorted(glob.glob(f\"{calib_dir}/calib_*.txt\"))\n\n# Ensure equal number of image and point cloud files\nmin_frames = min(len(image_files), len(point_files))\nimage_files = image_files[:min_frames]\npoint_files = point_files[:min_frames]\nprint(f\"Processing {min_frames} frames\")\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UUSS6Sv22_60","outputId":"1567270a-ad6a-4143-85af-1c0fd2dce65c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Processing 108 frames\n"]}],"execution_count":9},{"cell_type":"code","source":"# Load UFLD model\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel_path = \"/content/drive/MyDrive/CameraLidarFusion/culane_18.pth\"\nif not os.path.exists(model_path):\n    print(f\"❌ Model not found at {model_path}. Please upload culane_18.pth to CameraLidarFusion directory.\")\n    raise FileNotFoundError\nlane_detector = UltrafastLaneDetector(model_path, model_type=ModelType.CULANE, use_gpu=(device.type == \"cuda\"))\nlane_detector.model.to(device)\nprint(\"✅ UFLD Model loaded successfully!\")\n\n# Save UFLD model\nsaved_model_path = \"/content/drive/MyDrive/CameraLidarFusion/ulfd_culane_18.pth\"\ntorch.save(lane_detector.model.state_dict(), saved_model_path)\nprint(f\"✅ UFLD Model saved to: {saved_model_path}\")\n\n# Load YOLO and DeepSORT","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":257},"id":"H-Lb-NZr2_-R","outputId":"639c410a-15fa-434b-bb4f-e1703ab11b29"},"outputs":[{"output_type":"stream","name":"stdout","text":["❌ Model not found at /content/drive/MyDrive/CameraLidarFusion/culane_18.pth. Please upload culane_18.pth to CameraLidarFusion directory.\n"]},{"output_type":"error","ename":"FileNotFoundError","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-10-4f08be7bf612>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"❌ Model not found at {model_path}. Please upload culane_18.pth to CameraLidarFusion directory.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mlane_detector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUltrafastLaneDetector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mModelType\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCULANE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_gpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"cuda\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mlane_detector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: "]}],"execution_count":10},{"cell_type":"code","source":"# LiDAR2Camera class\nclass LiDAR2Camera:\n    def __init__(self, calib_dir):\n        calibs = self.read_all_calib_files(calib_dir)\n        self.P = np.reshape(calibs[\"P_rect_02\"], [3, 4])\n        R = np.reshape(calibs[\"R\"], [3, 3])\n        T = np.reshape(calibs[\"T\"], [3, 1])\n        self.V2C = np.hstack((R, T))\n        self.R0 = np.eye(3)\n        self.V2C_homo = np.vstack((self.V2C, [0, 0, 0, 1]))\n\n    def read_all_calib_files(self, calib_dir):\n        data = {}\n        for calib_file in glob.glob(f\"{calib_dir}/calib_*.txt\"):\n            with open(calib_file, \"r\") as f:\n                for line in f.readlines():\n                    line = line.rstrip()\n                    if len(line) == 0:\n                        continue\n                    key, value = line.split(\":\", 1)\n                    try:\n                        data[key.strip()] = np.array([float(x) for x in value.split()])\n                    except ValueError:\n                        continue\n        return data\n\n    def cart2hom(self, pts_3d):\n        return np.hstack((pts_3d, np.ones((pts_3d.shape[0], 1))))\n\n    def project_velo_to_image(self, pts_3d):\n        pts_3d_homo = self.cart2hom(pts_3d)\n        cam_coords = np.dot(self.V2C_homo, pts_3d_homo.T).T\n        mask = cam_coords[:, 2] > 0\n        cam_coords = cam_coords[mask]\n        img_coords = np.dot(self.P, cam_coords.T).T\n        pts_2d = img_coords[:, :2] / img_coords[:, 2:3]\n        return pts_2d, mask, cam_coords\n\n    def show_lidar_on_image(self, pts_3d, img):\n        pts_2d, mask, cam_coords = self.project_velo_to_image(pts_3d)\n        pts_3d = pts_3d[mask]\n        img_copy = img.copy()\n        depths = pts_3d[:, 0]\n        max_depth = 30.0\n        depths = np.nan_to_num(depths, nan=0.0, posinf=max_depth, neginf=0.0)\n        step = 5\n        for i, (u, v) in enumerate(pts_2d[::step]):\n            i_original = i * step\n            if 0 <= u < img.shape[1] and 0 <= v < img.shape[0]:\n                depth = depths[i_original]\n                if depth <= 0:\n                    continue\n                color_idx = min(depth / max_depth, 1.0)\n                if color_idx < 0.33:\n                    r = int(255 * (color_idx / 0.33))\n                    g = 255\n                    b = 0\n                elif color_idx < 0.66:\n                    r = 255\n                    g = int(255 - (color_idx - 0.33) / 0.33 * 90)\n                    b = 0\n                else:\n                    r = 255\n                    g = int(165 - (color_idx - 0.66) / 0.34 * 165)\n                    b = 0\n                color = (r, g, b)\n                cv2.circle(img_copy, (int(u), int(v)), 1, color, -1)\n        return img_copy, pts_2d, pts_3d\n","metadata":{"id":"pKhjXOs14YgC"},"outputs":[],"execution_count":11},{"cell_type":"code","source":"yolo_model = YOLO(\"yolov8m.pt\")\ndeep_sort = DeepSort(embedder=\"mobilenet\", max_age=30, nms_max_overlap=0.5)\n\n\n# Load KITTI LiDAR\ndef load_kitti_bin(file_path):\n    points = []\n    with open(file_path, \"rb\") as f:\n        while True:\n            byte = f.read(16)\n            if not byte:\n                break\n            x, y, z, intensity = struct.unpack(\"ffff\", byte)\n            points.append([x, y, z])\n    return np.array(points)\n\n# Path planning function\ndef plan_path(lanes_points, lanes_detected, detected_objects, lidar_points, current_speed_kmph=60):\n    path_suggestions = []\n    ego_lane_idx = lane_detector.get_ego_lane_index()\n\n    if lanes_detected[3]:\n        path_suggestions.append(\"✅ Plan to pull over to the rightmost lane.\")\n    else:\n        path_suggestions.append(\"❌ Rightmost lane not available. Stay in current lane.\")\n\n    for obj in detected_objects:\n        if obj[\"lane_idx\"] == ego_lane_idx:\n            safe_dist = compute_safe_distance(obj[\"class\"], current_speed_kmph)\n            if obj[\"center\"][0] < safe_dist:\n                path_suggestions.append(f\"⚠ Slow down: {obj['class']} ahead at {obj['center'][0]:.2f}m (safe distance: {safe_dist:.2f}m).\")\n\n    if len(lidar_points) > 0:\n        close_points = lidar_points[lidar_points[:, 0] < 10]\n        if len(close_points) > 50:\n            path_suggestions.append(\"⚠ Dense obstacles ahead. Reduce speed or change lane if safe.\")\n\n    return path_suggestions\n","metadata":{"id":"-tBscf1x6lQ5"},"outputs":[],"execution_count":12},{"cell_type":"code","source":"\n# BEV generation function\ndef create_birds_eye_view(points_3d, detected_objects, lane_points_3d, x_range=(-20, 50), y_range=(-30, 30), grid_size=0.1):\n    mask = (points_3d[:, 0] >= x_range[0]) & (points_3d[:, 0] <= x_range[1]) & \\\n           (points_3d[:, 1] >= y_range[0]) & (points_3d[:, 1] <= y_range[1])\n    points_3d = points_3d[mask]\n    x_bins = int((x_range[1] - x_range[0]) / grid_size)\n    y_bins = int((y_range[1] - y_range[0]) / grid_size)\n    bev_map = np.zeros((x_bins, y_bins))\n    for point in points_3d:\n        x, y = point[:2]\n        x_idx = int((x - x_range[0]) / grid_size)\n        y_idx = int((y - y_range[0]) / grid_size)\n        if 0 <= x_idx < x_bins and 0 <= y_idx < y_bins:\n            bev_map[x_idx, y_idx] += 1\n    bev_map = np.log1p(bev_map)\n\n    fig = plt.figure(figsize=(10, 10))\n    plt.imshow(bev_map.T, cmap='viridis', origin='lower', extent=[x_range[0], x_range[1], y_range[0], y_range[1]])\n    plt.scatter([0], [0], color='red', s=100, label='Vehicle', marker='^')\n\n    for obj in detected_objects:\n        x, y = obj[\"center\"][:2]\n        color = [c/255 for c in obj[\"color\"]]\n        plt.scatter(x, y, color=color, s=200, alpha=0.7)\n        plt.text(x, y+1, f\"{obj['class']} ({obj['confidence']*100:.1f}%)\", color='white')\n\n    for i, lane in enumerate(lane_points_3d):\n        if len(lane) > 0:\n            lane_np = np.array(lane)\n            plt.plot(lane_np[:, 0], lane_np[:, 1], color=[c/255 for c in lane_colors[i % len(lane_colors)]], linewidth=2, label=f'Lane {i+1}')\n\n    plt.xlabel('X (meters)')\n    plt.ylabel('Y (meters)')\n    plt.title(\"Bird's-Eye View with Lanes\")\n    plt.legend()\n    plt.grid(True)\n\n    # Convert plot to image\n    fig.canvas.draw()\n    bev_img = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)\n    bev_img = bev_img.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n    plt.close(fig)\n    return bev_img\n","metadata":{"id":"H4xMl8296lGk"},"outputs":[],"execution_count":13},{"cell_type":"code","source":"\n# Setup video writers\nperspective_output_path = \"/content/drive/MyDrive/CameraLidarFusion/perspective_output.mp4\"\nbev_output_path = \"/content/drive/MyDrive/CameraLidarFusion/bev_output.mp4\"\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nimage = cv2.imread(image_files[0])\nframe_width, frame_height = image.shape[1], image.shape[0]\nperspective_out = cv2.VideoWriter(perspective_output_path, fourcc, 10, (frame_width, frame_height))\nbev_out = cv2.VideoWriter(bev_output_path, fourcc, 10, (1000, 1000))  # BEV size matches plot\n\n# Initialize LiDAR2Camera\nlidar2cam = LiDAR2Camera(calib_dir)\n","metadata":{"id":"eTGTtLyz647Z"},"outputs":[],"execution_count":14},{"cell_type":"code","source":"# Process video frames\nprint(\"✅ Processing video sequence...\")\nfor frame_idx, (img_file, pt_file) in enumerate(zip(image_files, point_files)):\n    print(f\"Processing frame {frame_idx+1}/{min_frames}\")\n    image = cv2.cvtColor(cv2.imread(img_file), cv2.COLOR_BGR2RGB)\n    points_3d_full = load_kitti_bin(pt_file)\n\n    # Detect lanes\n    lane_img, fps = lane_detector.detect_lanes(image)\n    lanes_points = lane_detector.lanes_points\n    lanes_detected = lane_detector.lanes_detected\n\n    # Project LiDAR points\n    img_with_lidar, pts_2d, pts_3d = lidar2cam.show_lidar_on_image(points_3d_full, lane_img)\n\n    # Project lane points to 3D\n    lane_points_3d = []\n    for lane in lanes_points:\n        lane_3d = []\n        if len(lane) > 0:\n            for pt in lane:\n                u, v = pt\n                distances = np.sqrt((pts_2d[:, 0] - u)**2 + (pts_2d[:, 1] - v)**2)\n                if len(distances) > 0 and np.min(distances) < 5:\n                    idx = np.argmin(distances)\n                    x, y, z = pts_3d[idx]\n                    lane_3d.append([x, y, z])\n        lane_points_3d.append(lane_3d)\n\n    # YOLO detection with DeepSORT\n    yolo_results = yolo_model(image, imgsz=1280, conf=0.3)\n    detections = []\n    tracked_yolo_boxes = []\n    for det in yolo_results[0].boxes:\n        x1, y1, x2, y2 = map(int, det.xyxy[0].tolist())\n        conf = float(det.conf[0])\n        cls_id = int(det.cls[0])\n        class_name = yolo_model.names[cls_id]\n        if conf < 0.6 or class_name not in ['car', 'truck', 'person']:\n            continue\n        w, h = x2 - x1, y2 - y1\n        detections.append(([x1, y1, w, h], conf, cls_id))\n        tracked_yolo_boxes.append({\n            \"bbox\": [x1, y1, w, h],\n            \"cls_id\": cls_id,\n            \"center\": (x1 + w // 2, y1 + h // 2)\n        })\n\n    tracks = deep_sort.update_tracks(detections, frame=image)\n    detected_objects = []\n    for track in tracks:\n        if not track.is_confirmed():\n            continue\n        x1, y1, x2, y2 = track.to_ltrb()\n        w, h = x2 - x1, y2 - y1\n        cx = int(x1 + w / 2)\n        matched = None\n        for yolo_box in tracked_yolo_boxes:\n            yolo_cx, yolo_cy = yolo_box[\"center\"]\n            if abs(cx - yolo_cx) < 30:\n                matched = yolo_box\n                break\n        if not matched:\n            continue\n        x, y, w, h = matched[\"bbox\"]\n        cls_id = matched[\"cls_id\"]\n        class_name = yolo_model.names[cls_id]\n        color = (0, 255, 255) if class_name == \"car\" else (255, 255, 0) if class_name == \"truck\" else (0, 255, 0)\n        obj_points = []\n        for i, (u, v) in enumerate(pts_2d):\n            if x <= u <= x+w and y <= v <= y+h:\n                obj_points.append(pts_3d[i])\n        if obj_points:\n            obj_center = np.mean(obj_points, axis=0)\n            lane_idx = lane_detector.get_ego_lane_index()\n            detected_objects.append({\n                \"class\": class_name,\n                \"confidence\": matched.get(\"conf\", 0.6),\n                \"center\": obj_center,\n                \"color\": color,\n                \"lane_idx\": lane_idx\n            })\n            cv2.rectangle(img_with_lidar, (x, y), (x+w, y+h), color, 2)\n            cv2.putText(img_with_lidar, f\"{class_name}: {obj_center[0]:.2f}m\", (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)\n\n    # Plan path\n    path_suggestions = plan_path(lanes_points, lanes_detected, detected_objects, points_3d_full)\n\n    # Overlay path suggestions\n    for i, suggestion in enumerate(path_suggestions):\n        cv2.putText(img_with_lidar, suggestion, (10, 30 + i*30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n\n    # Generate BEV\n    bev_img = create_birds_eye_view(points_3d_full, detected_objects, lane_points_3d)\n\n    # Write to videos\n    perspective_out.write(cv2.cvtColor(img_with_lidar, cv2.COLOR_RGB2BGR))\n    bev_out.write(cv2.cvtColor(bev_img, cv2.COLOR_RGB2BGR))","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":255},"id":"hq5XwX5s64we","outputId":"b27604a6-a574-4b35-c6b5-e6e0e0c9b5b7"},"outputs":[{"output_type":"stream","name":"stdout","text":["✅ Processing video sequence...\n","Processing frame 1/108\n"]},{"output_type":"error","ename":"NameError","evalue":"name 'lane_detector' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-15-844932087b6d>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# Detect lanes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mlane_img\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlane_detector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetect_lanes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mlanes_points\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlane_detector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlanes_points\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mlanes_detected\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlane_detector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlanes_detected\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'lane_detector' is not defined"]}],"execution_count":15},{"cell_type":"code","source":"\n# Release video writers\nperspective_out.release()\nbev_out.release()\nprint(f\"✅ Videos saved to: {perspective_output_path} and {bev_output_path}\")\n\n# Print sample detected objects from last frame\nprint(\"Sample Detected Objects from Last Frame:\")\nfor obj in detected_objects:\n    x, y, z = obj[\"center\"]\n    print(f\"{obj['class']} ({obj['confidence']*100:.1f}%): (x={x:.2f}, y={y:.2f}, z={z:.2f})\")","metadata":{"id":"kGRrplCe7OxE"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"input_video_path = \"/kaggle/input/car-video/videoplayback.mp4\"\noutput_video_path = \"/kaggle/working/output_video.mp4\"\n\ncap = cv2.VideoCapture(input_video_path)\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = int(cap.get(cv2.CAP_PROP_FPS))\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_video_path, fourcc, fps, (frame_width, frame_height))\n\nif not cap.isOpened():\n    print(f\"❌ Cannot open video: {input_video_path}\")\n    exit()\n\nprint(\"✅ Processing video...\")\n\nCLASS_COLORS = {\n    0: (0, 0, 255),      # person\n    1: (0, 255, 0),      # bicycle\n    2: (255, 255, 255),  # car\n    3: (255, 255, 0),    # motorcycle\n    5: (255, 0, 255),    # bus\n    7: (0, 255, 255),    # truck\n    9: (0, 255, 0)       # traffic light\n}\nCLASS_NAMES = yolo_model.model.names\nSHOW_INFO_CLASSES = {0, 1, 2, 3, 5, 7}\n\n# Distance Estimator\ndef estimate_distance(bbox, focal_length=720, default_height=1.6):\n    x, y, w, h = bbox\n    if h <= 0:\n        return 999\n    return round((default_height * focal_length) / h, 1)\n\n# ✅ STEP 4: Lane Assignment\n# Lane 1 = rightmost\ndef assign_lane(x_center, lane_boundaries):\n    for i in range(len(lane_boundaries) - 1):\n        if lane_boundaries[i + 1] <= x_center <= lane_boundaries[i]:\n            return str(i + 1)\n    return \"_\"\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    #  Detect lanes (UFLD)\n    with torch.no_grad():\n        frame_with_lanes, fps = lane_detector.detect_lanes(frame)\n\n    if frame_with_lanes is None:\n        continue\n\n    #  Extract lane centers\n    all_lanes = lane_detector.lanes_points[:4]\n    valid_lanes = [lane for lane in all_lanes if len(lane) > 0]\n\n    if len(valid_lanes) >= 2:\n        lane_xs = sorted([np.median([pt[0] for pt in lane]) for lane in valid_lanes])[::-1]\n    else:\n        lane_xs = []\n\n    # YOLOv11 Detection\n    yolo_results = yolo_model(frame)[0]\n    detections = []\n    tracked_yolo_boxes = []  # To store matched YOLO boxes by track ID\n\n    for det in yolo_results.boxes:\n        x1, y1, x2, y2 = map(int, det.xyxy[0].tolist())\n        conf = float(det.conf[0])\n        cls_id = int(det.cls[0])\n        if conf < 0.6 or cls_id not in CLASS_COLORS:\n            continue\n        w, h = x2 - x1, y2 - y1\n        detections.append(([x1, y1, w, h], conf, cls_id))\n        tracked_yolo_boxes.append({\n            \"bbox\": [x1, y1, w, h],\n            \"cls_id\": cls_id,\n            \"center\": (x1 + w // 2, y1 + h // 2)\n        })\n\n\n    tracks = deep_sort.update_tracks(detections, frame=frame)\n\n    for track in tracks:\n        if not track.is_confirmed():\n            continue\n\n        track_id = track.track_id\n        x1, y1, x2, y2 = track.to_ltrb()\n        w, h = x2 - x1, y2 - y1\n        cx = int(x1 + w / 2)\n\n        # Match track to YOLO box (by center proximity)\n        matched = None\n        for yolo_box in tracked_yolo_boxes:\n            yolo_cx, yolo_cy = yolo_box[\"center\"]\n            if abs(cx - yolo_cx) < 30:  # simple proximity check\n                matched = yolo_box\n                break\n\n        if not matched:\n            continue\n\n        x, y, w, h = matched[\"bbox\"]\n        cls_id = matched[\"cls_id\"]\n        if cls_id not in SHOW_INFO_CLASSES:\n            continue\n\n        distance = estimate_distance([x, y, w, h])\n        lane_number = assign_lane(x + w // 2, lane_xs)\n        color = CLASS_COLORS[cls_id]\n\n        # Draw box and labels\n        cv2.rectangle(frame_with_lanes, (x, y), (x + w, y + h), color, 2)\n        cv2.putText(frame_with_lanes, f\"{distance:.1f}m\", (x + 5, y + h - 12),\n                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n        lane_text = f\"Lane {lane_number}\"\n        text_size = cv2.getTextSize(lane_text, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)[0]\n        cv2.putText(frame_with_lanes, lane_text, (x + w - text_size[0] - 5, y + h - 12),\n                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n\n    if frame_with_lanes.shape[:2] != (frame_height, frame_width):\n        frame_with_lanes = cv2.resize(frame_with_lanes, (frame_width, frame_height))\n\n    out.write(frame_with_lanes)\n\ncap.release()\nout.release()\ncv2.destroyAllWindows()\nprint(f\"✅ Processed video saved to: {output_video_path}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}